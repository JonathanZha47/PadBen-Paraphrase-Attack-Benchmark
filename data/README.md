# PadBenData: Text Classification Dataset

This repository contains a comprehensive dataset for text classification tasks, specifically designed for detecting different types of text generation and paraphrasing patterns. The dataset is organized into two main components: the original taxonomy data and task-specific datasets.

## Dataset Overview

### 1. Original Taxonomy Data (`data.json`)

The `data.json` file contains the foundational dataset with 5 distinct text types, each representing different text generation and paraphrasing scenarios:

#### Text Type Taxonomy:
- **Type 1 (Human Original Text)**: Original human-written text from various sources
- **Type 2 (LLM Generated Text)**: Text generated by large language models
- **Type 3 (Human Paraphrased Text)**: Human-created paraphrases of original text
- **Type 4 (LLM Paraphrased Original Text)**: LLM-generated paraphrases of original human text
- **Type 5 (LLM Paraphrased Generated Text)**: LLM-generated paraphrases of LLM-generated text
  - **5-1st**: First iteration of LLM paraphrasing
  - **5-3rd**: Third iteration of LLM paraphrasing

#### Data Structure:
Each entry contains:
- `idx`: Unique identifier
- `dataset_source`: Source dataset (e.g., "mrpc")
- `human_original_text(type1)`: Original human text
- `llm_generated_text(type2)`: LLM-generated text
- `human_paraphrased_text(type3)`: Human paraphrased text
- `llm_paraphrased_original_text(type4)-prompt-based`: LLM paraphrased original text
- `llm_paraphrased_generated_text(type5)-1st`: First iteration LLM paraphrase
- `llm_paraphrased_generated_text(type5)-3rd`: Third iteration LLM paraphrase

### 2. Task-Specific Datasets (`task_data/`)

The `task_data/` directory contains datasets organized for specific classification tasks, with different experimental setups and methodologies.

#### Directory Structure:
```
task_data/
├── tasks/
│   ├── single-sentence/          # Single sentence classification tasks
│   │   ├── exhaustive_method/    # Complete dataset processing
│   │   └── sampling_method/      # Balanced sampling approaches
│   │       ├── 30-70/           # 30% positive, 70% negative samples
│   │       ├── 50-50/           # 50% positive, 50% negative samples
│   │       └── 80-20/           # 80% positive, 20% negative samples
│   └── sentence-pair/           # Sentence pair classification tasks
│       ├── task1/               # Paraphrase source attribution
│       ├── task2/               # General text authorship detection
│       ├── task3/               # AI text laundering detection
│       ├── task4/               # Iterative paraphrase depth detection
│       └── task5/               # Original vs deep paraphrase attack detection
```

#### Task Descriptions:

1. **Task 1: Paraphrase Source Attribution without Context**
   - **Objective**: Distinguish between human paraphrased text (Type 3) and LLM paraphrased original text (Type 4)
   - **Label 0**: Human paraphrased text (Type 3)
   - **Label 1**: LLM paraphrased original text (Type 4)

2. **Task 2: General Text Authorship Detection**
   - **Objective**: Distinguish between human original text (Type 1) and LLM generated text (Type 2)
   - **Label 0**: Human original text (Type 1)
   - **Label 1**: LLM generated text (Type 2)

3. **Task 3: AI Text Laundering Detection**
   - **Objective**: Detect AI-generated text that has been paraphrased to appear more human-like
   - **Label 0**: LLM paraphrased original text (Type 4)
   - **Label 1**: LLM paraphrased generated text - 1st iteration (Type 5-1st)

4. **Task 4: Iterative Paraphrase Depth Detection**
   - **Objective**: Distinguish between different depths of iterative paraphrasing
   - **Label 0**: LLM paraphrased generated text - 1st iteration (Type 5-1st)
   - **Label 1**: LLM paraphrased generated text - 3rd iteration (Type 5-3rd)

5. **Task 5: Original vs Deep Paraphrase Attack Detection**
   - **Objective**: Detect deep paraphrase attacks by distinguishing original text from heavily paraphrased text
   - **Label 0**: Human original text (Type 1)
   - **Label 1**: LLM paraphrased generated text - 3rd iteration (Type 5-3rd)

#### Experimental Setups:

1. **Exhaustive Method**: Uses the complete dataset with all available samples
   - **Input samples**: 16,233
   - **Total output samples**: 162,330 (10x expansion ratio)
   - **Processing**: All samples processed for each task

2. **Sampling Method**: Balanced sampling with different class distributions
   - **30-70 Split**: 30% positive samples, 70% negative samples
   - **50-50 Split**: Balanced 50% positive, 50% negative samples
   - **80-20 Split**: 80% positive samples, 20% negative samples

#### Data Format:

Each task dataset contains JSON files with the following structure:
```json
[
  {
    "idx": 0,
    "sentence": "Sample text for classification",
    "label": 1
  }
]
```

#### Pipeline Summary Files:

Each experimental setup includes a `pipeline_summary.json` file containing:
- Pipeline status and processing statistics
- Task-specific results and validation status
- Sample counts and distribution information
- Processing times and configuration details

## Usage

### Loading the Original Taxonomy:
```python
import json

with open('data.json', 'r') as f:
    taxonomy_data = json.load(f)
```

### Loading Task-Specific Datasets:
```python
# Load a specific task dataset
with open('task_data/tasks/single-sentence/exhaustive_method/task1/task1_paraphrase_source_without_context.json', 'r') as f:
    task_data = json.load(f)
```

### Accessing Pipeline Information:
```python
# Load pipeline summary
with open('task_data/tasks/single-sentence/exhaustive_method/pipeline_summary.json', 'r') as f:
    pipeline_info = json.load(f)
```

## Dataset Statistics

- **Total original samples**: 16,233
- **Text types**: 5 distinct types
- **Tasks**: 5 classification tasks
- **Experimental setups**: 3 sampling methods + exhaustive method
- **Total processed samples**: Varies by setup (16,233 to 162,330 per task)

## Applications

This dataset is designed for research in:
- Text classification and authorship detection
- AI-generated text detection
- Paraphrase detection and analysis
- Machine learning model evaluation
- Natural language processing research

## File Naming Convention

- **Task files**: `task{N}_{task_description}.json`
- **Report files**: `task{N}_{task_description}_report.json`
- **Pipeline summaries**: `pipeline_summary.json` or `dynamic_pipeline_summary.json`
- **Dynamic tasks**: `dynamic_task{N}_{task_description}.json`

## Notes

- All datasets are in JSON format for easy integration with machine learning frameworks
- Labels are binary (0/1) for classification tasks
- Each sample includes an index for tracking and validation
- Pipeline summaries provide comprehensive metadata about data processing and validation

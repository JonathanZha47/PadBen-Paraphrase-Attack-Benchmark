# -*- coding: utf-8 -*-
"""padben_eval.ipynb

Automatically generated by Colab.
"""

#!/usr/bin/env python3
"""
OpenAI Client LLM Text Detection Evaluator
Purpose: Call LLMs via OpenAI client to detect AI vs Human text
Data format: {"text": str, "label": int, "idx": int}
Labels: 1 = LLM-generated, 0 = Human-written
"""

import json
import time
import os
import glob
import numpy as np
import argparse
from openai import OpenAI
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score, roc_curve
import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm
from prompt_templates import (
    get_single_sentence_messages,
    get_sentence_pair_messages,
    get_batch_messages
)

# =============================================================================
# HELPER FUNCTIONS FOR ADVANCED METRICS
# =============================================================================

def calculate_tpr_at_fpr(y_true, y_scores, fpr_thresholds=[0.01, 0.05, 0.10]):
    """
    Calculate True Positive Rate (TPR) at specific False Positive Rate (FPR) thresholds
    
    Args:
        y_true: True binary labels (0 or 1)
        y_scores: Prediction scores/probabilities (higher = more likely to be class 1)
        fpr_thresholds: List of FPR thresholds (e.g., [0.01, 0.05, 0.10] for 1%, 5%, 10%)
    
    Returns:
        Dictionary with TPR values at each FPR threshold
    """
    try:
        # Calculate ROC curve
        fpr, tpr, thresholds = roc_curve(y_true, y_scores)
        
        tpr_at_fpr = {}
        for fpr_thresh in fpr_thresholds:
            # Find the index where FPR is closest to the threshold
            idx = np.argmax(fpr >= fpr_thresh)
            if idx == 0 and fpr[0] > fpr_thresh:
                # If even the first point has FPR > threshold, use 0
                tpr_at_fpr[f"TPR@{fpr_thresh*100:.0f}%FPR"] = 0.0
            else:
                tpr_at_fpr[f"TPR@{fpr_thresh*100:.0f}%FPR"] = tpr[idx]
        
        return tpr_at_fpr
    except Exception as e:
        print(f"Warning: Could not calculate TPR@FPR metrics: {e}")
        return {f"TPR@{fpr_thresh*100:.0f}%FPR": 0.0 for fpr_thresh in fpr_thresholds}

def calculate_tpr_at_fpr_binary(y_true, y_pred, fpr_thresholds=[0.01, 0.05, 0.10]):
    """
    Calculate True Positive Rate (TPR) at specific False Positive Rate (FPR) thresholds
    for binary predictions. Since we only have binary predictions, we simulate different
    thresholds by adjusting the decision boundary.
    
    Args:
        y_true: True binary labels (0 or 1)
        y_pred: Binary predictions (0 or 1)
        fpr_thresholds: List of FPR thresholds (e.g., [0.01, 0.05, 0.10] for 1%, 5%, 10%)
    
    Returns:
        Dictionary with TPR values at each FPR threshold
    """
    try:
        # Calculate confusion matrix
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()
        
        # Calculate actual FPR and TPR
        actual_fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
        actual_tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        
        tpr_at_fpr = {}
        
        # For binary predictions, we can only achieve one operating point
        # We'll simulate different thresholds by adjusting the decision boundary
        for fpr_thresh in fpr_thresholds:
            if actual_fpr <= fpr_thresh:
                # If our actual FPR is below the threshold, we can use our actual TPR
                tpr_at_fpr[f"TPR@{fpr_thresh*100:.0f}%FPR"] = actual_tpr
            else:
                # If our actual FPR is above the threshold, we need to be more conservative
                # We'll estimate what TPR we could achieve if we reduced FPR to the threshold
                # This is a conservative estimate based on the trade-off
                estimated_tpr = actual_tpr * (fpr_thresh / actual_fpr) if actual_fpr > 0 else 0.0
                tpr_at_fpr[f"TPR@{fpr_thresh*100:.0f}%FPR"] = min(estimated_tpr, actual_tpr)
        
        return tpr_at_fpr
    except Exception as e:
        print(f"Warning: Could not calculate TPR@FPR metrics for binary predictions: {e}")
        return {f"TPR@{fpr_thresh*100:.0f}%FPR": 0.0 for fpr_thresh in fpr_thresholds}

# =============================================================================
# STEP 1: LOAD DATA
# =============================================================================

def load_data(file_path):
    """Load data from JSON file"""
    print(f"📂 Loading data from: {file_path}")

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"❌ Error reading file {file_path}: {e}")
        return []

    # Debug: Print data type and structure
    print(f"🔍 Data type: {type(data)}")
    if isinstance(data, dict):
        print(f"🔍 Dict keys: {list(data.keys())}")
        if len(data) > 0:
            first_key = list(data.keys())[0]
            print(f"🔍 First key '{first_key}' type: {type(data[first_key])}")
            if isinstance(data[first_key], list) and len(data[first_key]) > 0:
                print(f"🔍 First item in '{first_key}': {type(data[first_key][0])}")

    # Ensure data is a list
    if not isinstance(data, list):
        print(f"⚠️ Warning: Data is not a list, type: {type(data)}")
        if isinstance(data, dict):
            # If it's a dict, try to find the actual data
            if 'data' in data:
                data = data['data']
                print("🔍 Found 'data' key, using data['data']")
            elif 'samples' in data:
                data = data['samples']
                print("🔍 Found 'samples' key, using data['samples']")
            elif 'items' in data:
                data = data['items']
                print("🔍 Found 'items' key, using data['items']")
            else:
                print(f"❌ Cannot convert data to list. Keys: {list(data.keys()) if isinstance(data, dict) else 'N/A'}")
                # Try to use the first value if it's a list
                for key, value in data.items():
                    if isinstance(value, list):
                        print(f"🔍 Using key '{key}' which contains a list")
                        data = value
                        break
                else:
                    return []
        else:
            print(f"❌ Unexpected data type: {type(data)}")
            return []

    # Final check
    if not isinstance(data, list):
        print(f"❌ Still not a list after conversion: {type(data)}")
        return []

    print(f"✅ Loaded {len(data)} samples")

    # Check data format and show sample
    if len(data) > 0:
        print("\n📋 Sample data:")
        for i, sample in enumerate(data[:3]):
            print(f"  {i}: {sample}")

        # Check label distribution
        if isinstance(data[0], dict) and 'label' in data[0]:
            labels = [item['label'] for item in data]
            human_count = sum(1 for label in labels if label == 0)
            llm_count = sum(1 for label in labels if label == 1)

            print(f"\n📊 Label distribution:")
            print(f"  Human (0): {human_count} samples ({human_count/len(data)*100:.1f}%)")
            print(f"  LLM (1): {llm_count} samples ({llm_count/len(data)*100:.1f}%)")

    return data

def load_reduced_tasks_data(base_path=None):
    """Load all reduced tasks data files"""
    if base_path is None:
        # Try to find reduced_tasks directory relative to this script
        script_dir = os.path.dirname(os.path.abspath(__file__))
        parent_dir = os.path.dirname(script_dir)
        base_path = os.path.join(parent_dir, "reduced_tasks")
        
        # If not found, try current directory
        if not os.path.exists(base_path):
            base_path = "reduced_tasks"
    print(f"📂 Loading reduced tasks data from: {base_path}")
    
    all_data = {}
    
    # Skip exhaustive method - only focus on sampling method and sentence-pair tasks
    print("📋 Skipping exhaustive method - focusing on sampling method and sentence-pair tasks")
    
    # Load single-sentence sampling method data with different ratios
    sampling_base_path = os.path.join(base_path, "single-sentence", "sampling_method")
    
    # Available ratios based on actual directory structure
    ratios = ["20-80", "50-50", "70-30"]
    
    # Task file mappings (matching fast-detect-gpt structure)
    task_files = {
        'task1': 'reduced_dynamic_task1_paraphrase_source_without_context.json',
        'task2': 'reduced_dynamic_task2_general_text_authorship_detection.json',
        'task3': 'reduced_dynamic_task3_ai_text_laundering_detection.json',
        'task4': 'reduced_dynamic_task4_iterative_paraphrase_depth_detection.json',
        'task5': 'reduced_dynamic_task5_original_vs_deep_paraphrase_attack.json'
    }
    
    for ratio in ratios:
        sampling_path = os.path.join(sampling_base_path, ratio)
        if os.path.exists(sampling_path):
            print(f"📊 Loading sampling method data with {ratio} ratio")
            for task_num in range(1, 6):
                task_key = f"task{task_num}"
                task_dir = os.path.join(sampling_path, task_key)
                file_name = task_files[task_key]
                data_file = os.path.join(task_dir, file_name)
                
                if os.path.exists(data_file):
                    full_task_key = f"{task_key}_sampling_{ratio.replace('-', '_')}"
                    try:
                        data = load_data(data_file)
                        if data:  # Only add if data was successfully loaded
                            all_data[full_task_key] = data
                        else:
                            print(f"⚠️ Skipping {full_task_key} - no valid data loaded")
                    except Exception as e:
                        print(f"❌ Error loading {full_task_key}: {e}")
                        continue
                else:
                    print(f"⚠️ File not found: {data_file}")
        else:
            print(f"⚠️ Sampling ratio {ratio} not found, skipping")
    
    # Load sentence-pair data
    sentence_pair_path = os.path.join(base_path, "sentence-pair")
    
    # Sentence-pair file mappings (matching fast-detect-gpt structure)
    sentence_pair_files = {
        'task1': 'reduced_task1_paraphrase_source_without_context_sentence_pair.json',
        'task2': 'reduced_task2_general_text_authorship_detection_sentence_pair.json',
        'task3': 'reduced_task3_ai_text_laundering_detection_sentence_pair.json',
        'task4': 'reduced_task4_iterative_paraphrase_depth_detection_sentence_pair.json',
        'task5': 'reduced_task5_original_vs_deep_paraphrase_attack_sentence_pair.json'
    }
    
    for task_num in range(1, 6):
        task_key = f"task{task_num}"
        task_dir = os.path.join(sentence_pair_path, task_key)
        file_name = sentence_pair_files[task_key]
        data_file = os.path.join(task_dir, file_name)
        
        if os.path.exists(data_file):
            full_task_key = f"{task_key}_sentence_pair"
            try:
                data = load_data(data_file)
                if data:  # Only add if data was successfully loaded
                    all_data[full_task_key] = data
                else:
                    print(f"⚠️ Skipping {full_task_key} - no valid data loaded")
            except Exception as e:
                print(f"❌ Error loading {full_task_key}: {e}")
                continue
        else:
            print(f"⚠️ File not found: {data_file}")
    
    print(f"✅ Loaded {len(all_data)} task datasets")
    return all_data

# =============================================================================
# STEP 2: OPENAI CLIENT LLM DETECTOR CLASS
# =============================================================================

class OpenAILLMDetector:
    def __init__(self, api_key=None, base_url=None):
        # Load environment variables from .env file
        load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))
        
        # Use provided parameters or fall back to environment variables
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.base_url = base_url or os.getenv('BASE_URL')
        
        # Validate API key
        if not self.api_key or self.api_key == 'INSERT_YOUR_API_KEY_HERE':
            raise ValueError(
                "❌ OpenAI API key not found! Please:\n"
                "1. Create a .env file in the Models directory\n"
                "2. Add: OPENAI_API_KEY=INSERT_YOUR_API_KEY_HERE\n"
                "3. Optionally add: BASE_URL=INSERT_YOUR_BASE_URL_HERE"
            )
        
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        
        print(f"✅ OpenAI client initialized with base URL: {self.base_url}")
        print(f"🔑 API key loaded: {self.api_key[:10]}...{self.api_key[-4:] if len(self.api_key) > 14 else '***'}")

        # Model IDs for the specified models
        self.models = {
            "Claude-3.5-Haiku": "claude-3-5-haiku-20241022",
            "DeepSeek-V2.5": "deepseek-v2.5",
            "GLM-4.5": "GLM-4.5",
            "Kimi-K2-Instruct": "Kimi-K2-Instruct",
            "Qwen2.5-VL-72B": "Qwen2.5-VL-72B-Instruct",
            "Qwen3-32B": "Qwen3-32B",
            "GPT-OSS-120B": "openai/gpt-oss-120b",
            "Gemma-3-27B": "google/gemma-3-27b-it",
            "Llama-4-Scout-17B": "meta-llama/llama-4-scout-17b-16e-instruct",
            "Mistral-Nemo":"mistralai/mistral-nemo",
            "Llama-4-Maverick-17B":"meta-llama/llama-4-maverick-17b-128e-instruct-fp8",
            "Llama-3.3-70B-Instruct":"meta-llama/llama-3.3-70b-instruct",
            "WizardLM-2-8x22B":"microsoft/wizardlm-2-8x22b"
        }

        # Task information for display purposes
        self.task_info = {
            "task1": {
                "name": "Paraphrase Source Attribution",
                "description": "Distinguish between human and LLM paraphrases"
            },
            "task2": {
                "name": "General Text Authorship Detection",
                "description": "Distinguish between human original and LLM generated text"
            },
            "task3": {
                "name": "AI Text Laundering Detection",
                "description": "Distinguish between different levels of LLM paraphrasing"
            },
            "task4": {
                "name": "Iterative Paraphrase Depth Detection",
                "description": "Distinguish between different depths of iterative LLM paraphrasing"
            },
            "task5": {
                "name": "Original vs Deep Paraphrase Attack Detection",
                "description": "Distinguish human original text from sophisticated paraphrase attacks"
            }
        }

        # Initialize storage
        self.results = []

    def get_task_info_from_name(self, task_name):
        """Extract task number and type from task name"""
        # Parse task names like "task1_exhaustive", "task2_sampling", "task3_sentence_pair"
        parts = task_name.split('_')
        task_num = parts[0]  # e.g., "task1"
        
        if "sentence_pair" in task_name:
            return task_num, "sentence_pair"
        else:
            return task_num, "single_sentence"

    def get_messages_for_task(self, task_name, text=None, sentence1=None, sentence2=None):
        """Get the appropriate messages based on task type using multi-turn conversation format"""
        task_num, task_type = self.get_task_info_from_name(task_name)
        
        if task_type == "sentence_pair":
            return get_sentence_pair_messages(task_num, sentence1, sentence2)
        else:
            return get_single_sentence_messages(task_num, text)
    
    def get_prompt_for_task(self, task_name, text=None, sentence1=None, sentence2=None):
        """Legacy method for backward compatibility - converts messages to single prompt"""
        messages = self.get_messages_for_task(task_name, text, sentence1, sentence2)
        
        # Convert messages to single prompt for fallback
        prompt_parts = []
        for msg in messages:
            if msg["role"] == "system":
                prompt_parts.append(f"SYSTEM: {msg['content']}")
            elif msg["role"] == "user":
                prompt_parts.append(f"USER: {msg['content']}")
            elif msg["role"] == "assistant":
                prompt_parts.append(f"ASSISTANT: {msg['content']}")
        
        return "\n\n".join(prompt_parts)

    def call_openai_with_retry(self, model_id, task_name, text=None, sentence1=None, sentence2=None, max_retries=3):
        """Call OpenAI API with retry logic using multi-turn conversation format"""

        # Get task-specific messages with few-shot examples
        messages = self.get_messages_for_task(task_name, text=text, sentence1=sentence1, sentence2=sentence2)

        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=model_id,
                    messages=messages,
                    max_tokens=2,
                    temperature=0.0,
                    timeout=30
                )

                answer = response.choices[0].message.content.strip()

                # Parse response
                if answer == '0':
                    return 0
                elif answer == '1':
                    return 1
                elif '0' in answer and '1' not in answer:
                    return 0
                elif '1' in answer and '0' not in answer:
                    return 1
                else:
                    # Unclear response - retry with SAME model and SAME prompt
                    if attempt < max_retries - 1:
                        print(f"  🔄 {model_id} retry {attempt + 1}: '{answer}' unclear, retrying same model...")
                        time.sleep(1.0)  # Wait a bit longer before retry
                        continue
                    else:
                        print(f"  ❌ {model_id} final attempt failed: '{answer}', defaulting to 0")
                        return 0

            except Exception as e:
                error_str = str(e)
                if "rate_limit" in error_str.lower() or "429" in error_str:
                    print(f"  🔄 {model_id} rate limit, waiting and retrying...")
                    time.sleep(3)
                    continue
                elif "insufficient" in error_str.lower() or "quota" in error_str.lower():
                    print(f"  💳 {model_id} insufficient credits")
                    return None
                elif attempt < max_retries - 1:
                    print(f"  🔄 {model_id} retry {attempt + 1}: {str(e)[:50]}...")
                    time.sleep(2)
                    continue
                else:
                    print(f"  ❌ {model_id} final error: {e}")
                    return 0

        return 0

    def call_model_with_retry(self, model_name, model_id, task_name, text=None, sentence1=None, sentence2=None, max_retries=3):
        """
        Unified method to call any model with retry logic
        Returns: prediction (0=human, 1=AI), None for payment errors
        """
        return self.call_openai_with_retry(model_id, task_name, text=text, sentence1=sentence1, sentence2=sentence2, max_retries=max_retries)

    def call_openai_batch(self, model_id, task_name, batch_data, max_retries=3):
        """
        Call OpenAI API with batch processing for multiple samples using multi-turn format
        
        Args:
            model_id: Model identifier
            task_name: Task name for prompt selection
            batch_data: List of data items to process in batch
            max_retries: Maximum retry attempts
            
        Returns:
            List of predictions (0=human, 1=AI), None for payment errors
        """
        task_num, task_type = self.get_task_info_from_name(task_name)
        
        # Create batch messages with few-shot examples
        batch_messages = get_batch_messages(task_num, task_type, batch_data)
        
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=model_id,
                    messages=batch_messages,
                    max_tokens=50,  # More tokens for batch responses
                    temperature=0.0,
                    timeout=60  # Longer timeout for batch processing
                )

                answer = response.choices[0].message.content.strip()
                
                # Parse batch response
                predictions = self._parse_batch_response(answer, len(batch_data))
                if predictions is not None:
                    return predictions
                else:
                    if attempt < max_retries - 1:
                        print(f"  🔄 {model_id} batch retry {attempt + 1}: unclear response, retrying...")
                        time.sleep(1.0)
                        continue
                    else:
                        print(f"  ❌ {model_id} batch final attempt failed, defaulting to 0s")
                        return [0] * len(batch_data)

            except Exception as e:
                error_str = str(e)
                if "rate_limit" in error_str.lower() or "429" in error_str:
                    print(f"  🔄 {model_id} batch rate limit, waiting and retrying...")
                    time.sleep(3)
                    continue
                elif "insufficient" in error_str.lower() or "quota" in error_str.lower():
                    print(f"  💳 {model_id} batch insufficient credits")
                    return None
                elif attempt < max_retries - 1:
                    print(f"  🔄 {model_id} batch retry {attempt + 1}: {str(e)[:50]}...")
                    time.sleep(2)
                    continue
                else:
                    print(f"  ❌ {model_id} batch final error: {e}")
                    return [0] * len(batch_data)

        return [0] * len(batch_data)

    # Removed old batch prompt creation methods - now handled by prompt_templates.py

    def _parse_batch_response(self, response, expected_count):
        """Parse batch response and return list of predictions"""
        try:
            # Clean the response
            response = response.strip()
            
            # Remove any non-numeric characters except commas
            import re
            response = re.sub(r'[^0-9,]', '', response)
            
            # Split by comma and convert to integers
            predictions = []
            for pred_str in response.split(','):
                pred_str = pred_str.strip()
                if pred_str in ['0', '1']:
                    predictions.append(int(pred_str))
                else:
                    # If we can't parse, return None to trigger retry
                    return None
            
            # Check if we have the right number of predictions
            if len(predictions) == expected_count:
                return predictions
            else:
                print(f"  ⚠️ Expected {expected_count} predictions, got {len(predictions)}")
                return None
                
        except Exception as e:
            print(f"  ⚠️ Error parsing batch response: {e}")
            return None

    def evaluate_model(self, model_name, model_id, task_name, data, max_samples=None, batch_size=10):
        """Evaluate any model with batch processing (10 samples per batch)"""

        # Limit samples if specified
        test_data = data[:max_samples] if max_samples else data

        print(f"🧪 Testing {model_name} on {task_name} with {len(test_data)} samples (batch size: {batch_size})...")

        predictions = []
        true_labels = []
        errors = 0
        payment_errors = 0

        # Determine task type
        task_num, task_type = self.get_task_info_from_name(task_name)

        # Process data in batches
        total_batches = (len(test_data) + batch_size - 1) // batch_size
        pbar = tqdm(range(total_batches), desc=f"  {model_name} batches", 
                   bar_format="{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]")
        
        for batch_idx in pbar:
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(test_data))
            batch_data = test_data[start_idx:end_idx]
            
            # Prepare batch labels
            batch_labels = []
            for item in batch_data:
                if task_type == "sentence_pair":
                    sentence_pair = item.get('sentence_pair', [])
                    label_pair = item.get('label_pair', [])
                    if len(sentence_pair) >= 2 and len(label_pair) >= 2:
                        true_label = 1 if label_pair[1] == 1 else 0
                        batch_labels.append(true_label)
                    else:
                        batch_labels.append(0)  # Default for malformed pairs
                else:
                    batch_labels.append(item['label'])
            
            # Process batch
            batch_predictions = self.call_openai_batch(model_id, task_name, batch_data)
            
            if batch_predictions is None:  # Payment error
                payment_errors += 1
                pbar.set_postfix({"Status": "Payment error", "Errors": payment_errors})
                if payment_errors >= 3:  # Stop after 3 payment errors
                    pbar.set_postfix({"Status": "Too many payment errors - stopping"})
                    pbar.close()
                    print(f"  💳 Too many payment errors, stopping {model_name}")
                    return None
                # Use default predictions for this batch
                batch_predictions = [0] * len(batch_data)
            elif len(batch_predictions) != len(batch_data):
                print(f"  ⚠️ Batch size mismatch: expected {len(batch_data)}, got {len(batch_predictions)}")
                # Pad or truncate to match batch size
                if len(batch_predictions) < len(batch_data):
                    batch_predictions.extend([0] * (len(batch_data) - len(batch_predictions)))
                else:
                    batch_predictions = batch_predictions[:len(batch_data)]
            
            # Add batch results
            predictions.extend(batch_predictions)
            true_labels.extend(batch_labels)
            
            # Update progress bar with current accuracy
            if len(predictions) > 0:
                current_acc = sum(p == t for p, t in zip(predictions, true_labels)) / len(predictions)
                pbar.set_postfix({"Accuracy": f"{current_acc:.3f}", "Samples": len(predictions)})

        pbar.close()

        if len(predictions) == 0:
            print(f"  ❌ No valid predictions for {model_name}")
            return None

        # Calculate final metrics
        accuracy = accuracy_score(true_labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            true_labels, predictions, average='weighted', zero_division=0
        )

        cm = confusion_matrix(true_labels, predictions)
        
        # Calculate AUROC and TPR@FPR metrics
        try:
            # For AUROC, we need prediction probabilities, but we only have binary predictions
            # We'll use the predictions as scores (0 or 1) - this is a limitation
            auroc = roc_auc_score(true_labels, predictions)
            
            # Calculate TPR at specific FPR thresholds
            # For binary predictions, we need to create a proper ROC curve
            # We'll use the confusion matrix to calculate TPR@FPR properly
            tpr_metrics = calculate_tpr_at_fpr_binary(true_labels, predictions)
            
        except Exception as e:
            print(f"  ⚠️ Could not calculate AUROC/TPR@FPR for {model_name}: {e}")
            auroc = 0.0
            tpr_metrics = {"TPR@1%FPR": 0.0, "TPR@5%FPR": 0.0, "TPR@10%FPR": 0.0}

        result = {
            'model': model_name,
            'task': task_name,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auroc': auroc,
            'errors': errors,
            'payment_errors': payment_errors,
            'total_samples': len(predictions),
            'predictions': predictions,
            'true_labels': true_labels,
            'confusion_matrix': cm,
            'batch_size': batch_size,
            'total_batches': total_batches
        }
        
        # Add TPR@FPR metrics to result
        result.update(tpr_metrics)

        print(f"  ✅ {model_name} completed - Accuracy: {accuracy:.3f} (processed in {total_batches} batches)")
        return result

    def run_evaluation(self, task_name, data, max_samples=None, batch_size=10):
        """Run unified evaluation on all models via OpenAI client with batch processing"""
        print(f"🚀 Starting OpenAI Client LLM Detection Evaluation for {task_name}")
        print(f"📦 Using batch processing (batch size: {batch_size})")
        print("="*60)

        results = []

        # Test ALL models using OpenAI client with progress bar
        model_items = list(self.models.items())
        model_pbar = tqdm(model_items, desc="🚀 Models", 
                         bar_format="{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]")
        
        for model_name, model_id in model_pbar:
            try:
                model_pbar.set_description(f"🚀 Evaluating {model_name}")
                result = self.evaluate_model(model_name, model_id, task_name, data, max_samples, batch_size)

                if result is None:
                    model_pbar.set_postfix({"Status": f"Skipped {model_name}"})
                else:
                    results.append(result)
                    model_pbar.set_postfix({"Status": f"Completed {model_name}", "Accuracy": f"{result['accuracy']:.3f}"})

            except Exception as e:
                model_pbar.set_postfix({"Status": f"Failed {model_name}"})
                print(f"❌ Failed to evaluate {model_name}: {str(e)[:100]}")
                continue

        model_pbar.close()

        if len(results) == 0:
            print("❌ No successful evaluations. Check your API key and credits.")
        else:
            print(f"✅ Successfully evaluated {len(results)}/{len(self.models)} models")

        return results

    def run_reduced_tasks_evaluation(self, all_tasks_data, max_samples=None, batch_size=10, output_dir="output"):
        """Run evaluation on all reduced tasks with batch processing"""
        print("🚀 Starting Reduced Tasks Evaluation with Task-Specific Prompts")
        print(f"📦 Using batch processing (batch size: {batch_size})")
        print(f"📁 Output directory: {output_dir}")
        print("="*60)
        
        all_results = {}
        
        # Create progress bar for tasks
        task_items = list(all_tasks_data.items())
        task_pbar = tqdm(task_items, desc="📋 Tasks", 
                        bar_format="{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]")
        
        for task_name, task_data in task_pbar:
            task_pbar.set_description(f"📋 Task: {task_name}")
            print(f"\n📋 Evaluating Task: {task_name}")
            
            # Display task-specific information
            task_num, task_type = self.get_task_info_from_name(task_name)
            if task_num in self.task_info:
                task_info = self.task_info[task_num]
                print(f"📝 Task: {task_info['name']}")
                print(f"🎯 Goal: {task_info['description']}")
                print(f"🎨 Format: Multi-turn conversation with few-shot examples")
            
            print("-" * 40)
            
            task_results = self.run_evaluation(task_name, task_data, max_samples, batch_size)
            all_results[task_name] = task_results
            
            # Save results immediately after each task completion
            if task_results:
                print(f"💾 Saving results for {task_name}...")
                try:
                    # Save results using the current detector instance
                    self.save_results(task_results, task_data, task_name, output_dir)
                    print(f"✅ Results saved for {task_name}")
                except Exception as e:
                    print(f"⚠️ Warning: Could not save results for {task_name}: {e}")
                
                best_model = max(task_results, key=lambda x: x['accuracy'])
                print(f"🏆 Best model for {task_name}: {best_model['model']} ({best_model['accuracy']:.3f})")
                task_pbar.set_postfix({"Best": f"{best_model['model']} ({best_model['accuracy']:.3f})", "Saved": "✅"})
            else:
                task_pbar.set_postfix({"Status": "No results"})
        
        task_pbar.close()
        
        return all_results

    def print_results(self, results):
        """Print comprehensive results for all models"""
        print("="*60)
        print("📈 OPENAI CLIENT EVALUATION RESULTS")
        print("="*60)

        # Sort by accuracy
        results.sort(key=lambda x: x['accuracy'], reverse=True)

        print(f"{'Model':<20} {'Accuracy':<10} {'AUROC':<8} {'TPR@1%':<8} {'TPR@5%':<8} {'TPR@10%':<9} {'F1':<8} {'Errors':<8}")
        print("-"*100)

        for result in results:
            payment_info = f"({result.get('payment_errors', 0)} payment)" if result.get('payment_errors', 0) > 0 else ""
            tpr_1 = result.get('TPR@1%FPR', 0.0)
            tpr_5 = result.get('TPR@5%FPR', 0.0)
            tpr_10 = result.get('TPR@10%FPR', 0.0)
            
            print(f"{result['model']:<20} {result['accuracy']:<10.3f} "
                  f"{result['auroc']:<8.3f} {tpr_1:<8.3f} {tpr_5:<8.3f} "
                  f"{tpr_10:<9.3f} {result['f1']:<8.3f} {result['errors']:<8} {payment_info}")

        if results:
            print(f"\n🏆 Best Model: {results[0]['model']} with {results[0]['accuracy']:.1%} accuracy")

        # Confusion matrices
        print(f"\n📊 Confusion Matrices:")
        print("Format: [[Human→Human, Human→AI], [AI→Human, AI→AI]]")

        for result in results:
            cm = result['confusion_matrix']
            print(f"{result['model']}: {cm.tolist()}")

    def print_reduced_tasks_results(self, all_results):
        """Print comprehensive results for all reduced tasks"""
        print("="*80)
        print("📈 REDUCED TASKS EVALUATION SUMMARY")
        print("="*80)
        
        for task_name, results in all_results.items():
            if results:
                print(f"\n📋 Task: {task_name}")
                print("-" * 50)
                self.print_results(results)
                print()
        
        # Overall summary
        print("\n🏆 OVERALL BEST MODELS PER TASK:")
        print("-" * 50)
        for task_name, results in all_results.items():
            if results:
                best_model = max(results, key=lambda x: x['accuracy'])
                print(f"{task_name:<30}: {best_model['model']:<20} ({best_model['accuracy']:.3f})")

    def save_results(self, results, data, task_name, base_output_dir="output"):
        """Save results to organized CSV files with new metrics"""
        all_dfs = {}
        
        # Create base output directory
        script_dir = os.path.dirname(os.path.abspath(__file__))
        output_base = os.path.join(script_dir, base_output_dir)
        os.makedirs(output_base, exist_ok=True)

        for result in results:
            model_name = result['model']
            
            # Create model-specific directory
            model_dir = os.path.join(output_base, model_name)
            os.makedirs(model_dir, exist_ok=True)
            
            # Create labels directory for this model
            labels_dir = os.path.join(model_dir, "labels")
            os.makedirs(labels_dir, exist_ok=True)
            
            # Prepare detailed results
            rows = []
            predictions = result['predictions']
            true_labels = result['true_labels']

            for i, (pred, true) in enumerate(zip(predictions, true_labels)):
                if i < len(data):  # Make sure we don't exceed data length
                    text = data[i].get('text', data[i].get('sentence', ''))
                    rows.append({
                        'model': model_name,
                        'task': task_name,
                        'idx': data[i].get('idx', i),
                        'text': text,
                        'true_label': true,
                        'predicted_label': pred,
                        'correct': pred == true
                    })

            # Save detailed predictions
            predictions_df = pd.DataFrame(rows)
            predictions_file = os.path.join(model_dir, f"{task_name}_predictions.csv")
            predictions_df.to_csv(predictions_file, index=False)
            
            # Save labels data for later metric calculations
            labels_data = {
                'task': [task_name] * len(predictions),
                'model': [model_name] * len(predictions),
                'sample_idx': list(range(len(predictions))),
                'true_label': true_labels,
                'predicted_label': predictions,
                'correct': [p == t for p, t in zip(predictions, true_labels)]
            }
            
            labels_df = pd.DataFrame(labels_data)
            labels_file = os.path.join(labels_dir, f"{task_name}_labels.csv")
            labels_df.to_csv(labels_file, index=False)
            
            # Save summary metrics
            summary_data = {
                'model': [model_name],
                'task': [task_name],
                'accuracy': [result['accuracy']],
                'precision': [result['precision']],
                'recall': [result['recall']],
                'f1': [result['f1']],
                'auroc': [result['auroc']],
                'tpr_1_fpr': [result.get('TPR@1%FPR', 0.0)],
                'tpr_5_fpr': [result.get('TPR@5%FPR', 0.0)],
                'tpr_10_fpr': [result.get('TPR@10%FPR', 0.0)],
                'total_samples': [result['total_samples']],
                'errors': [result['errors']],
                'payment_errors': [result['payment_errors']]
            }
            
            summary_df = pd.DataFrame(summary_data)
            summary_file = os.path.join(model_dir, f"{task_name}_summary.csv")
            summary_df.to_csv(summary_file, index=False)
            
            all_dfs[model_name] = {
                'predictions': predictions_df,
                'summary': summary_df
            }
            
            print(f"💾 {model_name} results saved to: {model_dir}/")
            print(f"📊 {model_name} labels saved to: {labels_dir}/")

        return all_dfs

    def save_reduced_tasks_results(self, all_results, all_data, base_output_dir="output"):
        """Save all reduced tasks results to organized directories"""
        all_dfs = {}
        
        for task_name, results in all_results.items():
            if results and task_name in all_data:
                task_dfs = self.save_results(results, all_data[task_name], task_name, base_output_dir)
                all_dfs[task_name] = task_dfs
        
        # Create overall summary file
        script_dir = os.path.dirname(os.path.abspath(__file__))
        output_base = os.path.join(script_dir, base_output_dir)
        
        summary_rows = []
        for task_name, results in all_results.items():
            if results:
                for result in results:
                    summary_rows.append({
                        'task': task_name,
                        'model': result['model'],
                        'accuracy': result['accuracy'],
                        'precision': result['precision'],
                        'recall': result['recall'],
                        'f1': result['f1'],
                        'auroc': result['auroc'],
                        'tpr_1_fpr': result.get('TPR@1%FPR', 0.0),
                        'tpr_5_fpr': result.get('TPR@5%FPR', 0.0),
                        'tpr_10_fpr': result.get('TPR@10%FPR', 0.0),
                        'total_samples': result['total_samples'],
                        'errors': result['errors'],
                        'payment_errors': result['payment_errors']
                    })
        
        if summary_rows:
            overall_summary_df = pd.DataFrame(summary_rows)
            overall_summary_file = os.path.join(output_base, "overall_summary.csv")
            overall_summary_df.to_csv(overall_summary_file, index=False)
            print(f"📊 Overall summary saved to: {overall_summary_file}")
        
        # Create consolidated labels file for all models and tasks
        self._create_consolidated_labels_file(all_results, output_base)
        
        return all_dfs

    def _create_consolidated_labels_file(self, all_results, output_base):
        """Create a consolidated labels file for all models and tasks"""
        try:
            consolidated_rows = []
            
            for task_name, results in all_results.items():
                if results:
                    for result in results:
                        model_name = result['model']
                        predictions = result['predictions']
                        true_labels = result['true_labels']
                        
                        for i, (pred, true) in enumerate(zip(predictions, true_labels)):
                            consolidated_rows.append({
                                'task': task_name,
                                'model': model_name,
                                'sample_idx': i,
                                'true_label': true,
                                'predicted_label': pred,
                                'correct': pred == true
                            })
            
            if consolidated_rows:
                consolidated_df = pd.DataFrame(consolidated_rows)
                consolidated_file = os.path.join(output_base, "consolidated_labels.csv")
                consolidated_df.to_csv(consolidated_file, index=False)
                print(f"📊 Consolidated labels saved to: {consolidated_file}")
                
        except Exception as e:
            print(f"⚠️ Warning: Could not create consolidated labels file: {e}")

# =============================================================================
# STEP 3: TEST RETRY LOGIC
# =============================================================================

def test_all_models_retry_logic(api_key=None, base_url=None, max_samples=2):
    """
    Test ALL models with unified retry logic using task-specific prompts
    """
    print("🧪 Testing ALL Models with Task-Specific Prompts")
    print("="*60)

    # Load sample data from reduced tasks
    all_data = load_reduced_tasks_data()
    if not all_data:
        print("❌ No data loaded from reduced tasks")
        return False
    
    # Use first available task data for testing
    sample_task = list(all_data.keys())[0]
    data = all_data[sample_task]

    try:
        # Initialize detector (will load from .env if no parameters provided)
        detector = OpenAILLMDetector(api_key, base_url)
    except ValueError as e:
        print(str(e))
        return False

    print(f"🎯 Testing with task: {sample_task}")
    task_num, task_type = detector.get_task_info_from_name(sample_task)
    
    if task_num in detector.task_info:
        task_info = detector.task_info[task_num]
        print(f"📝 Task: {task_info['name']}")
        print(f"🎯 Goal: {task_info['description']}")
        print(f"🎨 Format: Multi-turn conversation with few-shot examples")

    # Test ALL models with progress bar
    model_items = list(detector.models.items())
    test_pbar = tqdm(model_items, desc="🔬 Testing Models", 
                    bar_format="{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]")
    
    for model_name, model_id in test_pbar:
        test_pbar.set_description(f"🔬 Testing {model_name}")
        print(f"\n🔬 Testing {model_name} ({model_id})...")
        print("-" * 50)

        sample_pbar = tqdm(data[:max_samples], desc=f"  {model_name} samples", leave=False,
                          bar_format="{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt}")
        
        for i, item in enumerate(sample_pbar):
            print(f"\nSample {i+1}:")
            
            if task_type == "sentence_pair":
                sentence_pair = item.get('sentence_pair', [])
                label_pair = item.get('label_pair', [])
                
                if len(sentence_pair) >= 2:
                    sentence1, sentence2 = sentence_pair[0], sentence_pair[1]
                    true_label = 1 if label_pair[1] == 1 else 0
                    
                    print(f"  Sentence 1: {sentence1[:60]}{'...' if len(sentence1) > 60 else ''}")
                    print(f"  Sentence 2: {sentence2[:60]}{'...' if len(sentence2) > 60 else ''}")
                    print(f"  True: {true_label} ({'Second is AI' if true_label == 1 else 'First is AI'})")

                    # Test prediction with task-specific prompt
                    pred = detector.call_model_with_retry(model_name, model_id, sample_task, 
                                                        sentence1=sentence1, sentence2=sentence2)
                else:
                    print(f"  ⚠️ Malformed sentence pair, skipping")
                    continue
            else:
                text = item.get('text', item.get('sentence', ''))
                true_label = item['label']
                
                print(f"  Text: {text[:60]}{'...' if len(text) > 60 else ''}")
            print(f"  True: {true_label} ({'Human' if true_label == 0 else 'AI'})")

                # Test prediction with task-specific prompt
            pred = detector.call_model_with_retry(model_name, model_id, sample_task, text=text)

            if pred is not None:
                if task_type == "sentence_pair":
                    print(f"  Pred: {pred} ({'Second is AI' if pred == 1 else 'First is AI'})")
                else:
                    print(f"  Pred: {pred} ({'Human' if pred == 0 else 'AI'})")
                    print(f"  {'✅ Correct' if pred == true_label else '❌ Wrong'}")
                sample_pbar.set_postfix({"Status": "✅ Correct" if pred == true_label else "❌ Wrong"})
            else:
                print(f"  ❌ Credit/Payment limit reached")
                print(f"⚠️  Stopping evaluation for {model_name}")
                sample_pbar.set_postfix({"Status": "Payment limit reached"})
                break

            time.sleep(0.8)  # Slower to see each step clearly
        
        sample_pbar.close()
        print(f"\n✅ {model_name} task-specific prompt test completed")
        time.sleep(1)  # Pause between models
    
    test_pbar.close()

    print(f"\n🎯 All {len(detector.models)} models tested with task-specific prompts!")
    return True

# =============================================================================
# STEP 4: MAIN EXECUTION FUNCTION
# =============================================================================

def main_openai_evaluation(api_key=None, base_url=None, max_samples=None, output_dir="output"):
    """
    Complete evaluation pipeline using OpenAI client on reduced tasks

    Args:
        api_key: Your API key (optional, will load from .env if not provided)
        base_url: Base URL for the API (optional, will load from .env if not provided)
        max_samples: Limit samples for testing (None for all data)
        output_dir: Output directory for results (default: "output")
    """

    # Load all reduced tasks data
    all_data = load_reduced_tasks_data()
    
    if not all_data:
        print("❌ No data loaded from reduced tasks")
        return {}, {}

    try:
        # Initialize detector (will load from .env if no parameters provided)
        detector = OpenAILLMDetector(api_key, base_url)
    except ValueError as e:
        print(str(e))
        return {}, {}

    # Run evaluation on all reduced tasks
    all_results = detector.run_reduced_tasks_evaluation(all_data, max_samples, batch_size=10, output_dir=output_dir)

    # Print and save results
    if all_results:
        detector.print_reduced_tasks_results(all_results)
        all_dfs = detector.save_reduced_tasks_results(all_results, all_data, output_dir)
        return all_results, all_dfs
    else:
        print("❌ No results to display - check API key and credits")
        return {}, {}

# =============================================================================
# STEP 5: ARGUMENT PARSING
# =============================================================================

def parse_arguments():
    """Parse command line arguments for model evaluation"""
    parser = argparse.ArgumentParser(
        description="OpenAI Client LLM Text Detection Evaluator",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Test specific model with limited samples (batch processing)
  python prompt_instucted_models.py --model "Claude-3.5-Haiku" --samples 10
  
  # Test all models with specific sample size and custom batch size
  python prompt_instucted_models.py --samples 50 --batch-size 5
  
  # Test specific model on all data with larger batches
  python prompt_instucted_models.py --model "DeepSeek-V2.5" --batch-size 20
  
  # Quick test with 2 samples per model (small batches)
  python prompt_instucted_models.py --samples 2 --batch-size 1
  
  # Test specific model on specific task with custom batch size
  python prompt_instucted_models.py --model "GLM-4.5" --samples 20 --task "task1_sampling_50_50" --batch-size 5
        """
    )
    
    parser.add_argument(
        '--model', 
        type=str, 
        choices=[
            "Claude-3.5-Haiku", "DeepSeek-V2.5", "GLM-4.5", 
            "Kimi-K2-Instruct", "Qwen2.5-VL-72B", "Qwen3-32B",
            "GPT-OSS-120B", "Gemma-3-27B", "Llama-4-Scout-17B", 
            "Mistral-Nemo", "Llama-4-Maverick-17B", "Llama-3.3-70B-Instruct", "WizardLM-2-8x22B", "all"
        ],
        default="all",
        help="Model to test (default: all models)"
    )
    
    parser.add_argument(
        '--samples', 
        type=int, 
        default=None,
        help="Number of samples to test (default: all available data)"
    )
    
    parser.add_argument(
        '--task',
        type=str,
        default=None,
        help="Specific task to test (e.g., 'task1_exhaustive', 'task2_sampling', 'task3_sentence_pair')"
    )
    
    parser.add_argument(
        '--api-key',
        type=str,
        default=None,
        help="OpenAI API key (overrides .env file)"
    )
    
    parser.add_argument(
        '--base-url',
        type=str,
        default=None,
        help="Base URL for API (overrides .env file)"
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default="output",
        help="Output directory for results (default: output)"
    )
    
    parser.add_argument(
        '--test-mode',
        action='store_true',
        help="Run in test mode (2 samples per model for quick testing)"
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help="Enable verbose output"
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10,
        help="Batch size for processing samples (default: 10)"
    )
    
    return parser.parse_args()

def run_evaluation_with_args(args):
    """
    Run evaluation based on parsed command line arguments
    
    Args:
        args: Parsed command line arguments
    """
    print("🚀 Starting Evaluation with Command Line Arguments")
    print("="*60)
    print(f"🎯 Model: {args.model}")
    print(f"📊 Samples: {args.samples if args.samples else 'All available'}")
    print(f"📋 Task: {args.task if args.task else 'All tasks'}")
    print(f"🔧 Test Mode: {'Yes' if args.test_mode else 'No'}")
    print(f"📦 Batch Size: {args.batch_size}")
    print(f"📁 Output: {args.output_dir}")
    print("="*60)
    
    # Load all reduced tasks data
    all_data = load_reduced_tasks_data()
    
    if not all_data:
        print("❌ No data loaded from reduced tasks")
        return {}, {}
    
    try:
        # Initialize detector with provided API credentials
        detector = OpenAILLMDetector(args.api_key, args.base_url)
    except ValueError as e:
        print(str(e))
        return {}, {}
    
    # Determine sample size
    sample_size = args.samples
    if args.test_mode:
        sample_size = 2
        print("🧪 Test mode enabled - using 2 samples per model")
    
    # Filter tasks if specific task requested
    if args.task:
        if args.task in all_data:
            all_data = {args.task: all_data[args.task]}
            print(f"📋 Testing specific task: {args.task}")
        else:
            print(f"❌ Task '{args.task}' not found. Available tasks:")
            for task_name in all_data.keys():
                print(f"  - {task_name}")
            return {}, {}
    
    # Filter models if specific model requested
    if args.model != "all":
        if args.model in detector.models:
            detector.models = {args.model: detector.models[args.model]}
            print(f"🎯 Testing specific model: {args.model}")
        else:
            print(f"❌ Model '{args.model}' not found. Available models:")
            for model_name in detector.models.keys():
                print(f"  - {model_name}")
            return {}, {}
    
    # Run evaluation with batch processing
    all_results = detector.run_reduced_tasks_evaluation(all_data, sample_size, args.batch_size, args.output_dir)
    
    # Print and save results
    if all_results:
        detector.print_reduced_tasks_results(all_results)
        all_dfs = detector.save_reduced_tasks_results(all_results, all_data, args.output_dir)
        return all_results, all_dfs
    else:
        print("❌ No results to display - check API key and credits")
        return {}, {}

# =============================================================================
# STEP 6: READY TO RUN - OPENAI CLIENT VERSION
# =============================================================================

# Load environment variables
load_dotenv(os.path.join(os.path.dirname(__file__), '.env'))

print("🎯 OpenAI Client LLM Detection Evaluator Ready!")
print("🔗 Configuration: Loading from .env file")
print(f"🔑 API Key: {'✅ Loaded from .env' if os.getenv('OPENAI_API_KEY') else '❌ Not found in .env'}")
print(f"🌐 Base URL: {os.getenv('BASE_URL', 'Not set')}")

print("\n🌟 Features:")
print("   • 🎯 Latest models: Claude-3.5-Haiku, DeepSeek-V2.5, GLM-4.5, Kimi-K2, Qwen2.5-VL, Qwen3-32B")
print("   • 📊 Evaluates on all reduced tasks automatically")
print("   • ⚡ Fast and reliable OpenAI client")
print("   • 🔄 Same-model retry logic for consistency")
print("   • 🎨 Multi-turn conversation prompts with few-shot examples")
print("   • 📦 Batch processing with sophisticated prompting strategies")

print("\n🚀 Choose your evaluation approach:")
print("1. 🧪 TEST ALL MODELS (2 samples each) - Quick test with task-specific prompts")
print("2. 🔥 FULL EVALUATION (All models, all tasks) - Complete evaluation with specialized prompts")

print("\n💡 Option 1 - Test All Models with Task-Specific Prompts:")
print("# test_all_models_retry_logic(max_samples=2)  # Uses .env file")

print("\n💡 Option 2 - Full Evaluation of All Models on All Reduced Tasks:")
print("# all_results, all_dfs = main_openai_evaluation(max_samples=20)  # Uses .env file")

print("\n✅ ADVANCED FEATURES:")
print("   • 🔐 Environment variable support (.env file)")
print("   • 🎨 Multi-turn conversation format with system/user/assistant roles")
print("   • 🎯 Few-shot examples tailored to each task type")
print("   • 🔄 Uses SAME model and SAME conversation format for all retries")
print("   • 📊 Tests ALL models on ALL reduced tasks")
print("   • ⚡ Sophisticated prompting strategies for better performance")
print("   • 🎭 Persona-based system messages for each task")
print("   • 📁 Automatically loads all reduced tasks data")

print("\n🎨 MULTI-TURN CONVERSATION PROMPTS:")
print("   • Task 1: Paraphrase detection expert with few-shot examples")
print("   • Task 2: AI-generated text detection specialist with demonstrations")
print("   • Task 3: AI text laundering detection with comparative examples")
print("   • Task 4: Iterative processing depth analysis with depth examples")
print("   • Task 5: Sophisticated attack detection with evasion examples")
print("   • Each task uses system/user/assistant format with 3-4 few-shot examples")
print("   • Batch processing maintains conversation format with examples")

print("\n🔐 SETUP INSTRUCTIONS:")
print("1. Create a .env file in the Models directory")
print("2. Add your API key: OPENAI_API_KEY=INSERT_YOUR_API_KEY_HERE")
print("3. Optionally set: BASE_URL=INSERT_YOUR_BASE_URL_HERE")

print("\n🎯 RECOMMENDED FIRST STEP:")
print("test_all_models_retry_logic(max_samples=2)")

print("\n💡 QUICK START - Test All Models Now:")
print("# Just run this line (after setting up .env):")
print('# test_all_models_retry_logic(max_samples=2)')

print("\n🎉 This will test all 9 models:")
print("   1. Claude-3.5-Haiku")
print("   2. DeepSeek-V2.5")
print("   3. GLM-4.5")
print("   4. Kimi-K2-Instruct")
print("   5. Qwen2.5-VL-72B-Instruct")
print("   6. Qwen3-32B")
print("   7. GPT-OSS-20B")
print("   8. Gemma-3-27B")
print("   9. Llama-4-Scout-17B")

print("\n📊 Reduced Tasks Included:")
print("   • Single-sentence sampling method with different ratios (Tasks 1-5)")
print("     - 20-80, 50-50, 70-30 ratios")
print("   • Sentence-pair tasks (Tasks 1-5)")
print("   • Exhaustive method skipped (focus on sampling and sentence-pair)")

print("\n🎨 CONVERSATION FORMAT EXAMPLES:")
print("   • System: 'You are an expert text analyst specializing in paraphrase detection...'")
print("   • User: 'Please analyze this text: [example]' → Assistant: '1'")
print("   • User: 'Please analyze this text: [example]' → Assistant: '0'")
print("   • User: 'Please analyze this text: [example]' → Assistant: '1'")
print("   • User: 'Please analyze this text: [actual_text]' → Assistant: [prediction]")
print("   • Each conversation includes 3-4 few-shot examples before the actual query")

print("\nEach model uses MULTI-TURN CONVERSATIONS with FEW-SHOT EXAMPLES!")
print("Ready to evaluate with sophisticated prompting strategies! 🚀")

print("\n📝 EXAMPLE .env FILE CONTENT:")
print("# Copy this to Models/.env and replace with your actual key")
print("OPENAI_API_KEY=INSERT_YOUR_API_KEY_HERE")
print("BASE_URL=INSERT_YOUR_BASE_URL_HERE")

print("\n🚀 NEW COMMAND LINE INTERFACE WITH BATCH PROCESSING:")
print("Use argument parsing for flexible evaluation with batch processing:")
print("python prompt_instucted_models.py --help")
print("\n📦 BATCH PROCESSING FEATURES:")
print("   • Process samples in batches (default: 10 samples per batch)")
print("   • Multi-turn conversation format maintained in batches")
print("   • Few-shot examples included in every batch request")
print("   • Customizable batch size with --batch-size argument")
print("   • Better error handling and retry logic for batches")

# =============================================================================
# MAIN EXECUTION WITH ARGUMENT PARSING
# =============================================================================

if __name__ == "__main__":
    # Parse command line arguments
    args = parse_arguments()
    
    # Run evaluation based on arguments
    all_results, all_dfs = run_evaluation_with_args(args)
    
    if all_results:
        print("\n✅ Evaluation completed successfully!")
        print(f"📊 Results saved to: {args.output_dir}")
    else:
        print("\n❌ Evaluation failed. Check your API key and credits.")